[{"tvm_func_name": "tuned_depthwise_convolution_op_float_i128_128_56_56_w128_1_3_3_o128_128_56_56_ws1_1_wd1_1_p1_1_kernel0", "op_type": "QuantizeDepthwiseConv2dNative", "parameters": {"input_shape": [8, 16, 112, 112], "filter_shape": [16, 1, 3, 3], "output_shape": [8, 16, 112, 112], "window_movement_strides": [1, 1], "window_dilation_strides": [1, 1], "padding_below_diff": [1, 1], "identifier_suffix": "BatchNormInferenceRelu", "identifier_prefix": "Quantize", "in_quantize_bit": 8, "out_quantize_bit": 8}, "code": "__global__ void DepthConvForwardNHWC(\n    float*  input0, float* input1, float* input2, float * input3, float *input4, float * output0) {\n\n    \n    uint8_t * bottom_data = reinterpret_cast<uint8_t*>(input0);\n    uint8_t * weight = reinterpret_cast<uint8_t*>(input1);\n    const int * bias =  reinterpret_cast<int*>(input2);\n    const int integer = (int)(*input3);\n    const int shift = (int)(*input4);\n    const int nthreads = 1605632;\n    const int channels = 16;\n    const int height = 112;\n    const int width = 112;\n    const int conved_height = 112;\n    const int conved_width = 112;\n\n    const int kernel_h = 3;\n    const int kernel_w = 3;\n    const int stride_h = 1;\n    const int stride_w = 1;\n    const int pad_h = 1;\n    const int pad_w = 1;\n    uint8_t * top_data = reinterpret_cast<uint8_t*>(output0);\n\n\n#pragma unroll\n        for (int index = blockIdx.x * blockDim.x * 4 + threadIdx.x * 4; \\\n            index < (nthreads); \\\n            index += blockDim.x * gridDim.x * 4){\n \n    // get nhwc\n    const int c = index % channels;\n    const int pw = (index / channels) % conved_width;\n    const int ph = (index / channels / conved_width) % conved_height;\n    const int n = index / channels / conved_width / conved_height;\n \n    // get range of height and width\n    int hstart = ph * stride_h - pad_h;\n    int wstart = pw * stride_w - pad_w;\n    int hend = min(hstart + kernel_h, height + pad_h);\n    int wend = min(wstart + kernel_w, width + pad_w);\n \n//      const int pool_size = (hend - hstart) * (wend - wstart);\n    hstart = max(hstart, 0);\n    wstart = max(wstart, 0);\n    hend = min(hend, height);\n    wend = min(wend, width);\n \n    int aveval = 0;\n    // uint8_t* bottom_slice = bottom_data + (n * channels + c) * height * width;\n    uint8_t* weight_slice =\n    weight + c * kernel_h * kernel_w;\n \n    int khstart=hend<kernel_h?kernel_h-hend:0;\n    int kwstart=wend<kernel_w?kernel_w-wend:0;\n \n    register uint8_t bottom_reg[4] = {0};\n    register uint8_t weight_reg[4] = {0};\n \n#pragma unroll\n    for(int h = hstart; h < hend; ++h) {\n    #pragma unroll\n        for(int w = wstart; w < wend; ++w) {\n            bottom_reg[w-wstart] = \n                bottom_data[n * height * width * channels+ h * width * channels + w * channels + c];\n            weight_reg[w-wstart] = weight_slice[(khstart+h-hstart) * kernel_w + (kwstart+w-wstart)];\n        }\n        int pack_val1 = reinterpret_cast<int*>(&(bottom_reg[0]))[0];\n        int pack_val2 = reinterpret_cast<int*>(&(weight_reg[0]))[0];\n        aveval = __dp4a(pack_val1, pack_val2, aveval);\n    }\n \n\n    aveval+=bias[c];\n\n    aveval = ((aveval * integer) >> shift);\n    top_data[index] = aveval > 0 ? (uint8_t)aveval:0;\n}\n}", "gridDim": [3136, 1, 1], "blockDim": [512, 1, 1], "dynamic_shared_memory": 0}]