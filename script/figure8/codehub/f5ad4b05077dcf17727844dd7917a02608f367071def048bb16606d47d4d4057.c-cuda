// GLOBALS: input0:float32[1], input1:float32[1], input2:float32[1], input4:float32[32, 512, 799], input5:float32[512, 512, 3] -> output0:float32[32, 512, 399]
// BACKEND: c-cuda (default)
// CONFIG: {"Toutput0:D0": [-1, 4, 2, 2], "Toutput0:D1": [-1, 4, 64, 1], "Toutput0:D2": [-1, 1, 1, 3], "Toutput0:R0": [-1, 1, 3], "Toutput0:R1": [-1, 2, 2], "Toutput0:RA": 1, "Toutput0:S": 3, "Toutput0:U": 0}
// COMPUTE_V1: - einstein_v2(" mediate0[N0, N1, N2] = input0[0] where N0 in 32, N1 in 512, N2 in 799;   mediate1[N0, N1, N2] = input1[0] where N0 in 32, N1 in 512, N2 in 799;   mediate2[N0, N1, N2] = input2[0] where N0 in 32, N1 in 512, N2 in 799;  mediate3[N0, N1, N2] = input4[N0, N1, N2] / mediate2[N0, N1, N2];mediate4[N0, N1, N2] = mediate3[N0, N1, N2].call(`erf`); mediate5[N0, N1, N2] = mediate4[N0, N1, N2] + mediate1[N0, N1, N2]; mediate6[N0, N1, N2] = input4[N0, N1, N2] * mediate5[N0, N1, N2];mediate7[N0, N1, N2] = mediate6[N0, N1, N2] * mediate0[N0, N1, N2]; output0[N, F, WO] +=! mediate7[N, C, -0 + KW + WO * 2] * input5[F, C, KW] where WO in 399; ", input_dict={ "input0" : { "dtype" : "float32", "shape" : [1]} ,  "input1" : { "dtype" : "float32", "shape" : [1]} ,  "input2" : { "dtype" : "float32", "shape" : [1]} ,  "input4" : { "dtype" : "float32", "shape" : [32, 512, 799]} ,  "input5" : { "dtype" : "float32", "shape" : [512, 512, 3]} }) ## @:


// ---------------------------------------------------------------------------
// LOCAL: template_op_kernel0 -- input0:float32[1], input1:float32[1], input2:float32[1], input4:float32[32, 512, 799], input5:float32[512, 512, 3] -> output0:float32[32, 512, 399]

#include <cuda_runtime.h>
#include <cuda_fp16.h>
#include <mma.h>

#ifndef __CUDA_COMMON_MACRO__
#define __CUDA_COMMON_MACRO__

#define __ITEM_0_OF__(v) (v).x
#define __ITEM_1_OF__(v) (v).y
#define __ITEM_2_OF__(v) (v).z
#define __ITEM_3_OF__(v) (v).w

#define __STORE_ITEM_0__(t, out, ido, in, idi) *(t*)(out + ido) = *(t*)(in + idi)
#define __STORE_ITEM_1__(t, out, ido, in, idi)
#define __STORE_ITEM_2__(t, out, ido, in, idi)
#define __STORE_ITEM_3__(t, out, ido, in, idi)

#define MAKE_VEC4_OP(type) \
  __forceinline__ __device__ type operator+(const type &l, const type &r) { return make_##type(l.x + r.x, l.y + r.y, l.z + r.z, l.w + r.w); } \
  __forceinline__ __device__ type operator-(const type &l, const type &r) { return make_##type(l.x - r.x, l.y - r.y, l.z - r.z, l.w - r.w); } \
  __forceinline__ __device__ type operator*(const type &l, const type &r) { return make_##type(l.x * r.x, l.y * r.y, l.z * r.z, l.w * r.w); } \
  __forceinline__ __device__ type operator/(const type &l, const type &r) { return make_##type(l.x / r.x, l.y / r.y, l.z / r.z, l.w / r.w); } \
  __forceinline__ __device__ type operator%(const type &l, const type &r) { return make_##type(l.x % r.x, l.y % r.y, l.z % r.z, l.w % r.w); }
#define MAKE_VEC2_OP(type) \
  __forceinline__ __device__ type operator+(const type &l, const type &r) { return make_##type(l.x + r.x, l.y + r.y); } \
  __forceinline__ __device__ type operator-(const type &l, const type &r) { return make_##type(l.x - r.x, l.y - r.y); } \
  __forceinline__ __device__ type operator*(const type &l, const type &r) { return make_##type(l.x * r.x, l.y * r.y); } \
  __forceinline__ __device__ type operator/(const type &l, const type &r) { return make_##type(l.x / r.x, l.y / r.y); } \
  __forceinline__ __device__ type operator%(const type &l, const type &r) { return make_##type(l.x % r.x, l.y % r.y); }

MAKE_VEC4_OP(int4)
MAKE_VEC2_OP(int2)

__forceinline__ __device__ __half max(const __half a, const __half b) { return a > b ? a : b; }
__forceinline__ __device__ __half min(const __half a, const __half b) { return a < b ? a : b; }

#endif


extern "C" __global__ __launch_bounds__(128) void template_op_kernel0(float* __restrict__ input0, float* __restrict__ input1, float* __restrict__ input2, float* __restrict__ input4, float* __restrict__ input5, float* __restrict__ output0) {
  // [thread_extent] blockIdx.x = 532
  // [thread_extent] threadIdx.x = 128
  float output0_local[96];
  #pragma unroll
  for (int N_c_inner_init = 0; N_c_inner_init < 2; ++N_c_inner_init) {
    #pragma unroll
    for (int WO_c_inner_init = 0; WO_c_inner_init < 3; ++WO_c_inner_init) {
      #pragma unroll
      for (int vthread_s = 0; vthread_s < 16; ++vthread_s) {
        output0_local[((((vthread_s * 6) + (N_c_inner_init * 3)) + WO_c_inner_init))] = 0.000000e+00f;
      }
    }
  }
  for (int C_outer_outer = 0; C_outer_outer < 128; ++C_outer_outer) {
    __shared__ float mediate7_shared[448];
    __syncthreads();
    #pragma unroll
    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 4; ++ax0_ax1_fused_ax2_fused_outer_outer) {
  // [thread_extent] threadIdx.x = 128
      if (((ax0_ax1_fused_ax2_fused_outer_outer * 128) + ((int)threadIdx.x)) < 448) {
        mediate7_shared[(((ax0_ax1_fused_ax2_fused_outer_outer * 128) + ((int)threadIdx.x)))] = ((input4[((((((((((int)blockIdx.x) / 266) * 6545408) + ((((ax0_ax1_fused_ax2_fused_outer_outer * 128) + ((int)threadIdx.x)) / 28) * 409088)) + (C_outer_outer * 3196)) + (((((ax0_ax1_fused_ax2_fused_outer_outer * 128) + ((int)threadIdx.x)) % 28) / 7) * 799)) + ((((int)blockIdx.x) % 133) * 6)) + (((ax0_ax1_fused_ax2_fused_outer_outer * 128) + ((int)threadIdx.x)) % 7)))] * (erf((input4[((((((((((int)blockIdx.x) / 266) * 6545408) + ((((ax0_ax1_fused_ax2_fused_outer_outer * 128) + ((int)threadIdx.x)) / 28) * 409088)) + (C_outer_outer * 3196)) + (((((ax0_ax1_fused_ax2_fused_outer_outer * 128) + ((int)threadIdx.x)) % 28) / 7) * 799)) + ((((int)blockIdx.x) % 133) * 6)) + (((ax0_ax1_fused_ax2_fused_outer_outer * 128) + ((int)threadIdx.x)) % 7)))] / input2[(0)])) + input1[(0)])) * input0[(0)]);
      }
    }
    __shared__ float input5_shared[3072];
    #pragma unroll
    for (int ax0_ax1_fused_ax2_fused_outer_outer1 = 0; ax0_ax1_fused_ax2_fused_outer_outer1 < 24; ++ax0_ax1_fused_ax2_fused_outer_outer1) {
  // [thread_extent] threadIdx.x = 128
      input5_shared[(((ax0_ax1_fused_ax2_fused_outer_outer1 * 128) + ((int)threadIdx.x)))] = input5[(((((((((int)blockIdx.x) % 266) / 133) * 393216) + ((((ax0_ax1_fused_ax2_fused_outer_outer1 * 128) + ((int)threadIdx.x)) / 12) * 1536)) + (C_outer_outer * 12)) + (((ax0_ax1_fused_ax2_fused_outer_outer1 * 128) + ((int)threadIdx.x)) % 12)))];
    }
    __syncthreads();
    for (int C_outer_inner = 0; C_outer_inner < 2; ++C_outer_inner) {
      for (int C_inner = 0; C_inner < 2; ++C_inner) {
        #pragma unroll
        for (int KW = 0; KW < 3; ++KW) {
          #pragma unroll
          for (int N_c_inner = 0; N_c_inner < 2; ++N_c_inner) {
            #pragma unroll
            for (int WO_c_inner = 0; WO_c_inner < 3; ++WO_c_inner) {
              #pragma unroll
              for (int vthread_s1 = 0; vthread_s1 < 16; ++vthread_s1) {
                output0_local[((((vthread_s1 * 6) + (N_c_inner * 3)) + WO_c_inner))] = (output0_local[((((vthread_s1 * 6) + (N_c_inner * 3)) + WO_c_inner))] + (mediate7_shared[(((((((((vthread_s1 >> 2) * 112) + ((((int)threadIdx.x) >> 6) * 56)) + (N_c_inner * 28)) + (C_outer_inner * 14)) + (C_inner * 7)) + (WO_c_inner * 2)) + KW))] * input5_shared[(((((((vthread_s1 & 3) * 768) + ((((int)threadIdx.x) & 63) * 12)) + (C_outer_inner * 6)) + (C_inner * 3)) + KW))]));
              }
            }
          }
        }
      }
    }
  }
  for (int N_inner = 0; N_inner < 2; ++N_inner) {
    for (int WO_inner = 0; WO_inner < 3; ++WO_inner) {
      for (int vthread_s2 = 0; vthread_s2 < 16; ++vthread_s2) {
        output0[(((((((((((((int)blockIdx.x) / 266) * 3268608) + ((vthread_s2 >> 2) * 817152)) + ((((int)threadIdx.x) >> 6) * 408576)) + (N_inner * 204288)) + (((((int)blockIdx.x) % 266) / 133) * 102144)) + ((vthread_s2 & 3) * 25536)) + ((((int)threadIdx.x) & 63) * 399)) + ((((int)blockIdx.x) % 133) * 3)) + WO_inner))] = output0_local[((((vthread_s2 * 6) + (N_inner * 3)) + WO_inner))];
      }
    }
  }
}

// Saved Perf = 3.112900e-03 sec / run; Step Produced = 1000; Planned Steps = 1000;
// Antares Tuning Completed in 1000 steps.